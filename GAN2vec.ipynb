{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 : CMU-SE Dataset\n",
    "# 현재 Mode Collapse 문제 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\notebook2\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.activations import softmax\n",
    "from keras.objectives import binary_crossentropy as bce\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.distributions import RelaxedOneHotCategorical as gumbel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./train.txt\",sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ingore length <7 sentence,put it in length=7, split sentence 0:7 which length >7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in raw_data[\"text\"]:\n",
    "    split_text = i.split(\" \")\n",
    "    if len(split_text) <7:\n",
    "        pass\n",
    "    elif len(split_text) == 7:\n",
    "        data.append(split_text)\n",
    "    else:\n",
    "        data.append(split_text[0:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make word2vec model, dimension = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(data,size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change sentence to vector stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_to_word_vec = np.zeros(shape=(len(data),7,64))\n",
    "for sentence_index, i in enumerate(data):\n",
    "    temp_list = np.zeros(shape=(7,64))\n",
    "    for idx, j in enumerate(i):\n",
    "        try:\n",
    "            temp_list[idx] = np.array([model.wv.get_vector(j)])\n",
    "        except:\n",
    "            temp_list[idx] = np.array([model.wv.get_vector(\"<unk>\")])\n",
    "    temp_list = np.reshape(temp_list,(1,7,64))\n",
    "    sentence_to_word_vec[sentence_index] = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39812, 7, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_word_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add one dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_to_word_vec = np.expand_dims(sentence_to_word_vec,axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gumbel distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_loss(x, x_hat):\n",
    "    q_y = K.reshape(logits_y, (-1, N, M))\n",
    "    q_y = softmax(q_y)\n",
    "    log_q_y = K.log(q_y + 1e-20)\n",
    "    kl_tmp = q_y * (log_q_y - K.log(1.0/M))\n",
    "    KL = K.sum(kl_tmp, axis=(1, 2))\n",
    "    elbo = data_dim * bce(x, x_hat) - KL \n",
    "    return elbo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make GAN(based on DCGAN, infoGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN2vec():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.sentence_length = 7\n",
    "        self.word_dimension = 64\n",
    "        self.channels = 1\n",
    "        self.sentence_shape = (self.sentence_length, self.word_dimension, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(lr=0.0001, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        sentence = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(sentence)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Reshape((1,1,512)))\n",
    "        model.add(Conv2DTranspose(256,kernel_size=(3,16),strides=2))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2DTranspose(1,kernel_size=(3,34),strides=2))\n",
    "        model.add(Reshape((7,64,1)))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        sentence = model(noise)\n",
    "\n",
    "        return Model(noise, sentence)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(256, kernel_size=(3,64), input_shape=self.sentence_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(128, kernel_size=(5,1)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "        sentence = Input(shape=self.sentence_shape)\n",
    "        validity = model(sentence)\n",
    "\n",
    "        return Model(sentence, validity)\n",
    "    \n",
    "    \n",
    "    def pretrain_D(self, epochs, batch_size = 128):\n",
    "        X_train = sentence_to_word_vec\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        valid = valid*0.9\n",
    "        fake = fake+0.1\n",
    "        print(\"pretraining D\")\n",
    "        for epoch in range(epochs):\n",
    "            print(\"{}epochs\".format(epoch))\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            sentences = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_sentences = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(sentences, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_sentences, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        \n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        X_train = sentence_to_word_vec\n",
    "\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        valid = valid*0.9\n",
    "        fake = fake+0.1\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.show_sentence(epoch)\n",
    "\n",
    "    def show_sentence(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_sentence = GAN2vec.generator.predict(noise)\n",
    "        test = np.squeeze(gen_sentence)\n",
    "        for i in test:\n",
    "            sentence = \"\"\n",
    "            for j in i:\n",
    "                temp = model.wv.similar_by_vector(j)\n",
    "                sentence=sentence+temp[0][0]+\" \"\n",
    "            print(sentence)\n",
    "            \n",
    "    def predict(self):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_sentence = GAN2vec.generator.predict(noise)\n",
    "        test = np.squeeze(gen_sentence)\n",
    "        sentence_list = []\n",
    "        for i in test:\n",
    "            sentence = \"\"\n",
    "            for j in i:\n",
    "                temp = model.wv.similar_by_vector(j)\n",
    "                sentence=sentence+temp[0][0]+\" \"\n",
    "            sentence_list.append(sentence)\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 5, 1, 256)         49408     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 5, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 1, 1, 128)         163968    \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,505\n",
      "Trainable params: 213,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "reshape_29 (Reshape)         (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_29 (Conv2DT (None, 3, 16, 256)        6291712   \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 3, 16, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_30 (Conv2DT (None, 7, 64, 1)          26113     \n",
      "_________________________________________________________________\n",
      "reshape_30 (Reshape)         (None, 7, 64, 1)          0         \n",
      "=================================================================\n",
      "Total params: 6,371,585\n",
      "Trainable params: 6,370,561\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "pretraining D\n",
      "0epochs\n",
      "1epochs\n",
      "2epochs\n",
      "3epochs\n",
      "4epochs\n",
      "5epochs\n",
      "6epochs\n",
      "7epochs\n",
      "8epochs\n",
      "9epochs\n",
      "10epochs\n",
      "11epochs\n",
      "12epochs\n",
      "13epochs\n",
      "14epochs\n",
      "15epochs\n",
      "16epochs\n",
      "17epochs\n",
      "18epochs\n",
      "19epochs\n",
      "20epochs\n",
      "21epochs\n",
      "22epochs\n",
      "23epochs\n",
      "24epochs\n",
      "25epochs\n",
      "26epochs\n",
      "27epochs\n",
      "28epochs\n",
      "29epochs\n",
      "30epochs\n",
      "31epochs\n",
      "32epochs\n",
      "33epochs\n",
      "34epochs\n",
      "35epochs\n",
      "36epochs\n",
      "37epochs\n",
      "38epochs\n",
      "39epochs\n",
      "40epochs\n",
      "41epochs\n",
      "42epochs\n",
      "43epochs\n",
      "44epochs\n",
      "45epochs\n",
      "46epochs\n",
      "47epochs\n",
      "48epochs\n",
      "49epochs\n",
      "50epochs\n",
      "51epochs\n",
      "52epochs\n",
      "53epochs\n",
      "54epochs\n",
      "55epochs\n",
      "56epochs\n",
      "57epochs\n",
      "58epochs\n",
      "59epochs\n",
      "60epochs\n",
      "61epochs\n",
      "62epochs\n",
      "63epochs\n",
      "64epochs\n",
      "65epochs\n",
      "66epochs\n",
      "67epochs\n",
      "68epochs\n",
      "69epochs\n",
      "70epochs\n",
      "71epochs\n",
      "72epochs\n",
      "73epochs\n",
      "74epochs\n",
      "75epochs\n",
      "76epochs\n",
      "77epochs\n",
      "78epochs\n",
      "79epochs\n",
      "80epochs\n",
      "81epochs\n",
      "82epochs\n",
      "83epochs\n",
      "84epochs\n",
      "85epochs\n",
      "86epochs\n",
      "87epochs\n",
      "88epochs\n",
      "89epochs\n",
      "90epochs\n",
      "91epochs\n",
      "92epochs\n",
      "93epochs\n",
      "94epochs\n",
      "95epochs\n",
      "96epochs\n",
      "97epochs\n",
      "98epochs\n",
      "99epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\notebook2\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.326508, acc.: 0.00%] [G loss: 2.454106]\n",
      "too too from too from too from \n",
      "too too from too too too from \n",
      "too too from too too too from \n",
      "too too too too from too too \n",
      "too too from too from too too \n",
      "too too from too from too from \n",
      "too too from too from too from \n",
      "too too from too from too too \n",
      "too too from too from too from \n",
      "too too from too from too too \n",
      "too too from too from too too \n",
      "too too from too too too from \n",
      "too too from too from too too \n",
      "too too from too from too too \n",
      "too too from too from too too \n",
      "too too from too from too from \n",
      "too too from too too too from \n",
      "too too from too too too too \n",
      "too too from too from too from \n",
      "too too from too too too too \n",
      "too too from too too too from \n",
      "too too from too from too too \n",
      "too too from too from too from \n",
      "too too from too too too from \n",
      "too too from too from too too \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\notebook2\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 1.445808, acc.: 0.00%] [G loss: 0.335599]\n",
      "2 [D loss: 1.575082, acc.: 0.00%] [G loss: 0.339523]\n",
      "3 [D loss: 1.255063, acc.: 0.00%] [G loss: 0.429417]\n",
      "4 [D loss: 1.309397, acc.: 0.00%] [G loss: 0.449867]\n",
      "5 [D loss: 1.355660, acc.: 0.00%] [G loss: 0.402738]\n",
      "6 [D loss: 1.348910, acc.: 0.00%] [G loss: 0.386225]\n",
      "7 [D loss: 1.306595, acc.: 0.00%] [G loss: 0.391359]\n",
      "8 [D loss: 1.253108, acc.: 0.00%] [G loss: 0.395709]\n",
      "9 [D loss: 1.252030, acc.: 0.00%] [G loss: 0.397544]\n",
      "10 [D loss: 1.214879, acc.: 0.00%] [G loss: 0.391851]\n",
      "11 [D loss: 1.210647, acc.: 0.00%] [G loss: 0.387123]\n",
      "12 [D loss: 1.219715, acc.: 0.00%] [G loss: 0.387498]\n",
      "13 [D loss: 1.189231, acc.: 0.00%] [G loss: 0.388302]\n",
      "14 [D loss: 1.178473, acc.: 0.00%] [G loss: 0.388694]\n",
      "15 [D loss: 1.180424, acc.: 0.00%] [G loss: 0.385494]\n",
      "16 [D loss: 1.213764, acc.: 0.00%] [G loss: 0.388563]\n",
      "17 [D loss: 1.158913, acc.: 0.00%] [G loss: 0.388805]\n",
      "18 [D loss: 1.174827, acc.: 0.00%] [G loss: 0.384279]\n",
      "19 [D loss: 1.185552, acc.: 0.00%] [G loss: 0.385288]\n",
      "20 [D loss: 1.210360, acc.: 0.00%] [G loss: 0.390559]\n",
      "21 [D loss: 1.182459, acc.: 0.00%] [G loss: 0.389054]\n",
      "22 [D loss: 1.158795, acc.: 0.00%] [G loss: 0.383226]\n",
      "23 [D loss: 1.196947, acc.: 0.00%] [G loss: 0.383494]\n",
      "24 [D loss: 1.202017, acc.: 0.00%] [G loss: 0.386121]\n",
      "25 [D loss: 1.182553, acc.: 0.00%] [G loss: 0.385668]\n",
      "26 [D loss: 1.218455, acc.: 0.00%] [G loss: 0.386878]\n",
      "27 [D loss: 1.211483, acc.: 0.00%] [G loss: 0.389288]\n",
      "28 [D loss: 1.182406, acc.: 0.00%] [G loss: 0.385061]\n",
      "29 [D loss: 1.243595, acc.: 0.00%] [G loss: 0.386186]\n",
      "30 [D loss: 1.200496, acc.: 0.00%] [G loss: 0.385861]\n",
      "31 [D loss: 1.192238, acc.: 0.00%] [G loss: 0.378572]\n",
      "32 [D loss: 1.226306, acc.: 0.00%] [G loss: 0.381687]\n",
      "33 [D loss: 1.235273, acc.: 0.00%] [G loss: 0.387960]\n",
      "34 [D loss: 1.218883, acc.: 0.00%] [G loss: 0.387791]\n",
      "35 [D loss: 1.242003, acc.: 0.00%] [G loss: 0.384374]\n",
      "36 [D loss: 1.218525, acc.: 0.00%] [G loss: 0.381084]\n",
      "37 [D loss: 1.205605, acc.: 0.00%] [G loss: 0.379050]\n",
      "38 [D loss: 1.239452, acc.: 0.00%] [G loss: 0.381416]\n",
      "39 [D loss: 1.239253, acc.: 0.00%] [G loss: 0.384423]\n",
      "40 [D loss: 1.196134, acc.: 0.00%] [G loss: 0.380657]\n",
      "41 [D loss: 1.268624, acc.: 0.00%] [G loss: 0.385012]\n",
      "42 [D loss: 1.215665, acc.: 0.00%] [G loss: 0.384508]\n",
      "43 [D loss: 1.244383, acc.: 0.00%] [G loss: 0.381197]\n",
      "44 [D loss: 1.187088, acc.: 0.00%] [G loss: 0.378361]\n",
      "45 [D loss: 1.239860, acc.: 0.00%] [G loss: 0.376899]\n",
      "46 [D loss: 1.257120, acc.: 0.00%] [G loss: 0.386414]\n",
      "47 [D loss: 1.218523, acc.: 0.00%] [G loss: 0.386741]\n",
      "48 [D loss: 1.271326, acc.: 0.00%] [G loss: 0.384188]\n",
      "49 [D loss: 1.230820, acc.: 0.00%] [G loss: 0.379347]\n",
      "50 [D loss: 1.231854, acc.: 0.00%] [G loss: 0.377184]\n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory guys broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "<s> factory university broadway broadway abroad happened \n",
      "51 [D loss: 1.253410, acc.: 0.00%] [G loss: 0.377421]\n",
      "52 [D loss: 1.267998, acc.: 0.00%] [G loss: 0.384659]\n",
      "53 [D loss: 1.258751, acc.: 0.00%] [G loss: 0.386461]\n",
      "54 [D loss: 1.284667, acc.: 0.00%] [G loss: 0.381553]\n",
      "55 [D loss: 1.307658, acc.: 0.00%] [G loss: 0.384469]\n",
      "56 [D loss: 1.252475, acc.: 0.00%] [G loss: 0.381395]\n",
      "57 [D loss: 1.297760, acc.: 0.00%] [G loss: 0.378927]\n",
      "58 [D loss: 1.274086, acc.: 0.00%] [G loss: 0.380276]\n",
      "59 [D loss: 1.257141, acc.: 0.00%] [G loss: 0.377677]\n",
      "60 [D loss: 1.280196, acc.: 0.00%] [G loss: 0.379039]\n",
      "61 [D loss: 1.293372, acc.: 0.00%] [G loss: 0.380764]\n",
      "62 [D loss: 1.279006, acc.: 0.00%] [G loss: 0.383426]\n",
      "63 [D loss: 1.270376, acc.: 0.00%] [G loss: 0.378405]\n",
      "64 [D loss: 1.329304, acc.: 0.00%] [G loss: 0.379413]\n",
      "65 [D loss: 1.290120, acc.: 0.00%] [G loss: 0.380166]\n",
      "66 [D loss: 1.318327, acc.: 0.00%] [G loss: 0.383318]\n",
      "67 [D loss: 1.296854, acc.: 0.00%] [G loss: 0.379227]\n",
      "68 [D loss: 1.343366, acc.: 0.00%] [G loss: 0.380032]\n",
      "69 [D loss: 1.294994, acc.: 0.00%] [G loss: 0.379857]\n",
      "70 [D loss: 1.306871, acc.: 0.00%] [G loss: 0.376191]\n",
      "71 [D loss: 1.318452, acc.: 0.00%] [G loss: 0.377241]\n",
      "72 [D loss: 1.274189, acc.: 0.00%] [G loss: 0.375420]\n",
      "73 [D loss: 1.329631, acc.: 0.00%] [G loss: 0.378994]\n",
      "74 [D loss: 1.275294, acc.: 0.00%] [G loss: 0.376861]\n",
      "75 [D loss: 1.306152, acc.: 0.00%] [G loss: 0.375555]\n",
      "76 [D loss: 1.336652, acc.: 0.00%] [G loss: 0.378420]\n",
      "77 [D loss: 1.339134, acc.: 0.00%] [G loss: 0.383621]\n",
      "78 [D loss: 1.366289, acc.: 0.00%] [G loss: 0.382659]\n",
      "79 [D loss: 1.368055, acc.: 0.00%] [G loss: 0.379751]\n",
      "80 [D loss: 1.324961, acc.: 0.00%] [G loss: 0.373503]\n",
      "81 [D loss: 1.308396, acc.: 0.00%] [G loss: 0.368947]\n",
      "82 [D loss: 1.329717, acc.: 0.00%] [G loss: 0.374598]\n",
      "83 [D loss: 1.305291, acc.: 0.00%] [G loss: 0.377824]\n",
      "84 [D loss: 1.302589, acc.: 0.00%] [G loss: 0.376040]\n",
      "85 [D loss: 1.314456, acc.: 0.00%] [G loss: 0.373389]\n",
      "86 [D loss: 1.322464, acc.: 0.00%] [G loss: 0.372050]\n",
      "87 [D loss: 1.345378, acc.: 0.00%] [G loss: 0.378723]\n",
      "88 [D loss: 1.337588, acc.: 0.00%] [G loss: 0.379204]\n",
      "89 [D loss: 1.321181, acc.: 0.00%] [G loss: 0.373790]\n",
      "90 [D loss: 1.328144, acc.: 0.00%] [G loss: 0.369257]\n",
      "91 [D loss: 1.355236, acc.: 0.00%] [G loss: 0.376857]\n",
      "92 [D loss: 1.324070, acc.: 0.00%] [G loss: 0.374966]\n",
      "93 [D loss: 1.315813, acc.: 0.00%] [G loss: 0.371934]\n",
      "94 [D loss: 1.323667, acc.: 0.00%] [G loss: 0.371166]\n",
      "95 [D loss: 1.331298, acc.: 0.00%] [G loss: 0.374601]\n",
      "96 [D loss: 1.340821, acc.: 0.00%] [G loss: 0.376085]\n",
      "97 [D loss: 1.345993, acc.: 0.00%] [G loss: 0.374749]\n",
      "98 [D loss: 1.349227, acc.: 0.00%] [G loss: 0.374254]\n",
      "99 [D loss: 1.334814, acc.: 0.00%] [G loss: 0.372707]\n",
      "100 [D loss: 1.378849, acc.: 0.00%] [G loss: 0.375921]\n",
      "<s> cart lose broadway dressed huge measurements \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "<s> cart lose broadway dressed alone before \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "<s> cart lose broadway dressed alone before \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "<s> cart lose broadway dressed alone before \n",
      "<s> cart lose broadway dressed alone before \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "<s> cart lose broadway dressed huge measurements \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "<s> cart lose ski dressed alone measurements \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "<s> cart lose broadway dressed huge before \n",
      "<s> cart lose broadway dressed alone before \n",
      "<s> cart lose broadway dressed huge measurements \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "<s> cart lose broadway dressed huge measurements \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "<s> cart lose broadway dressed alone measurements \n",
      "101 [D loss: 1.338374, acc.: 0.00%] [G loss: 0.371652]\n",
      "102 [D loss: 1.386986, acc.: 0.00%] [G loss: 0.373187]\n",
      "103 [D loss: 1.347483, acc.: 0.00%] [G loss: 0.375359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 [D loss: 1.331580, acc.: 0.00%] [G loss: 0.372098]\n",
      "105 [D loss: 1.363930, acc.: 0.00%] [G loss: 0.370034]\n",
      "106 [D loss: 1.371752, acc.: 0.00%] [G loss: 0.372988]\n",
      "107 [D loss: 1.400984, acc.: 0.00%] [G loss: 0.374049]\n",
      "108 [D loss: 1.383749, acc.: 0.00%] [G loss: 0.371449]\n",
      "109 [D loss: 1.412013, acc.: 0.00%] [G loss: 0.371879]\n",
      "110 [D loss: 1.386787, acc.: 0.00%] [G loss: 0.368448]\n",
      "111 [D loss: 1.439536, acc.: 0.00%] [G loss: 0.371090]\n",
      "112 [D loss: 1.386048, acc.: 0.00%] [G loss: 0.369385]\n",
      "113 [D loss: 1.382264, acc.: 0.00%] [G loss: 0.363095]\n",
      "114 [D loss: 1.440922, acc.: 0.00%] [G loss: 0.365020]\n",
      "115 [D loss: 1.430380, acc.: 0.00%] [G loss: 0.369424]\n",
      "116 [D loss: 1.424918, acc.: 0.00%] [G loss: 0.367795]\n",
      "117 [D loss: 1.421468, acc.: 0.00%] [G loss: 0.366491]\n",
      "118 [D loss: 1.407643, acc.: 0.00%] [G loss: 0.364175]\n",
      "119 [D loss: 1.467343, acc.: 0.00%] [G loss: 0.367784]\n",
      "120 [D loss: 1.412907, acc.: 0.00%] [G loss: 0.368845]\n",
      "121 [D loss: 1.456857, acc.: 0.00%] [G loss: 0.369445]\n",
      "122 [D loss: 1.414020, acc.: 0.00%] [G loss: 0.367213]\n",
      "123 [D loss: 1.438792, acc.: 0.00%] [G loss: 0.365268]\n",
      "124 [D loss: 1.423098, acc.: 0.00%] [G loss: 0.363236]\n",
      "125 [D loss: 1.435705, acc.: 0.00%] [G loss: 0.365087]\n",
      "126 [D loss: 1.414251, acc.: 0.00%] [G loss: 0.366675]\n",
      "127 [D loss: 1.433024, acc.: 0.00%] [G loss: 0.365919]\n",
      "128 [D loss: 1.445233, acc.: 0.00%] [G loss: 0.363947]\n",
      "129 [D loss: 1.466051, acc.: 0.00%] [G loss: 0.367162]\n",
      "130 [D loss: 1.425054, acc.: 0.00%] [G loss: 0.365631]\n",
      "131 [D loss: 1.467659, acc.: 0.00%] [G loss: 0.365321]\n",
      "132 [D loss: 1.488259, acc.: 0.00%] [G loss: 0.368005]\n",
      "133 [D loss: 1.460352, acc.: 0.00%] [G loss: 0.367198]\n",
      "134 [D loss: 1.456482, acc.: 0.00%] [G loss: 0.364643]\n",
      "135 [D loss: 1.490625, acc.: 0.00%] [G loss: 0.363419]\n",
      "136 [D loss: 1.428542, acc.: 0.00%] [G loss: 0.363030]\n",
      "137 [D loss: 1.499178, acc.: 0.00%] [G loss: 0.365872]\n",
      "138 [D loss: 1.436453, acc.: 0.00%] [G loss: 0.364407]\n",
      "139 [D loss: 1.519253, acc.: 0.00%] [G loss: 0.368685]\n",
      "140 [D loss: 1.475878, acc.: 0.00%] [G loss: 0.364943]\n",
      "141 [D loss: 1.446124, acc.: 0.00%] [G loss: 0.359957]\n",
      "142 [D loss: 1.496098, acc.: 0.00%] [G loss: 0.365398]\n",
      "143 [D loss: 1.479774, acc.: 0.00%] [G loss: 0.368084]\n",
      "144 [D loss: 1.482417, acc.: 0.00%] [G loss: 0.363785]\n",
      "145 [D loss: 1.488869, acc.: 0.00%] [G loss: 0.365238]\n",
      "146 [D loss: 1.480550, acc.: 0.00%] [G loss: 0.365523]\n",
      "147 [D loss: 1.495023, acc.: 0.00%] [G loss: 0.363530]\n",
      "148 [D loss: 1.539642, acc.: 0.00%] [G loss: 0.366517]\n",
      "149 [D loss: 1.520962, acc.: 0.00%] [G loss: 0.367627]\n",
      "150 [D loss: 1.496347, acc.: 0.00%] [G loss: 0.361678]\n",
      "<s> money went picked then through ahead \n",
      "<s> new went picked then through again \n",
      "<s> new went picked then through again \n",
      "<s> money went picked then through again \n",
      "<s> money went picked then through again \n",
      "<s> money went picked then through again \n",
      "<s> money went picked then through again \n",
      "<s> money went picked then through ahead \n",
      "<s> new went picked then through again \n",
      "<s> money went picked then through again \n",
      "<s> new went picked then through ahead \n",
      "<s> money went picked then through ahead \n",
      "<s> money went picked then through again \n",
      "<s> new went picked then through again \n",
      "<s> money went picked then through again \n",
      "<s> money went picked then through again \n",
      "<s> money went picked then through again \n",
      "<s> money went picked then through again \n",
      "<s> new went picked then through again \n",
      "<s> money went picked then through again \n",
      "<s> money went picked then through again \n",
      "<s> new went picked then through ahead \n",
      "<s> money went picked then through again \n",
      "<s> money went picked then through again \n",
      "<s> new went picked then through again \n",
      "151 [D loss: 1.528344, acc.: 0.00%] [G loss: 0.362576]\n",
      "152 [D loss: 1.517269, acc.: 0.00%] [G loss: 0.367059]\n",
      "153 [D loss: 1.500984, acc.: 0.00%] [G loss: 0.362314]\n",
      "154 [D loss: 1.524255, acc.: 0.00%] [G loss: 0.363577]\n",
      "155 [D loss: 1.470268, acc.: 0.00%] [G loss: 0.363507]\n",
      "156 [D loss: 1.502627, acc.: 0.00%] [G loss: 0.363569]\n",
      "157 [D loss: 1.493155, acc.: 0.00%] [G loss: 0.365947]\n",
      "158 [D loss: 1.473293, acc.: 0.00%] [G loss: 0.364251]\n",
      "159 [D loss: 1.482457, acc.: 0.00%] [G loss: 0.364573]\n",
      "160 [D loss: 1.486491, acc.: 0.00%] [G loss: 0.362997]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-d380d7bb01ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mGAN2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGAN2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mGAN2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrain_D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mGAN2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-61-bfaf28d6cccc>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;31m# Train the generator (wants discriminator to mistake images as real)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;31m# Plot the progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\notebook2\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\notebook2\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\notebook2\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\notebook2\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "GAN2vec = GAN2vec()\n",
    "GAN2vec.pretrain_D(epochs = 100)\n",
    "GAN2vec.train(epochs=4000, batch_size=32, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    t = GAN2vec.predict()\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

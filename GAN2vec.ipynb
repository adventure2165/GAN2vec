{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 : CMU-SE Dataset\n",
    "# 현재 Mode Collapse 문제 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\notebook2\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.activations import softmax\n",
    "from keras.objectives import binary_crossentropy as bce\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.distributions import RelaxedOneHotCategorical as gumbel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./train.txt\",sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ingore length <7 sentence,put it in length=7, split sentence 0:7 which length >7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in raw_data[\"text\"]:\n",
    "    split_text = i.split(\" \")\n",
    "    if len(split_text) <7:\n",
    "        pass\n",
    "    elif len(split_text) == 7:\n",
    "        data.append(split_text)\n",
    "    else:\n",
    "        data.append(split_text[0:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make word2vec model, dimension = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(data,size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change sentence to vector stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_to_word_vec = np.zeros(shape=(len(data),7,64))\n",
    "for sentence_index, i in enumerate(data):\n",
    "    temp_list = np.zeros(shape=(7,64))\n",
    "    for idx, j in enumerate(i):\n",
    "        try:\n",
    "            temp_list[idx] = np.array([model.wv.get_vector(j)])\n",
    "        except:\n",
    "            temp_list[idx] = np.array([model.wv.get_vector(\"<unk>\")])\n",
    "    temp_list = np.reshape(temp_list,(1,7,64))\n",
    "    sentence_to_word_vec[sentence_index] = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39812, 7, 64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_word_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add one dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_to_word_vec = np.expand_dims(sentence_to_word_vec,axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make GAN(based on DCGAN, infoGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN2vec():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.sentence_length = 7\n",
    "        self.word_dimension = 64\n",
    "        self.channels = 1\n",
    "        self.sentence_shape = (self.sentence_length, self.word_dimension, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(lr=0.0001, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        sentence = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(sentence)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Reshape((1,1,512)))\n",
    "        model.add(Conv2DTranspose(256,kernel_size=(3,16),strides=2))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2DTranspose(1,kernel_size=(3,34),strides=2))\n",
    "        model.add(Reshape((7,64,1)))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        sentence = model(noise)\n",
    "\n",
    "        return Model(noise, sentence)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(256, kernel_size=(3,64), input_shape=self.sentence_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(128, kernel_size=(5,1)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "        sentence = Input(shape=self.sentence_shape)\n",
    "        validity = model(sentence)\n",
    "\n",
    "        return Model(sentence, validity)\n",
    "    \n",
    "    \n",
    "    def pretrain_D(self, epochs, batch_size = 128):\n",
    "        X_train = sentence_to_word_vec\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        valid = valid*0.9\n",
    "        fake = fake+0.1\n",
    "        print(\"pretraining D\")\n",
    "        for epoch in range(epochs):\n",
    "            print(\"{}epochs\".format(epoch))\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            sentences = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_sentences = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(sentences, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_sentences, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        \n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        X_train = sentence_to_word_vec\n",
    "\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        valid = valid*0.9\n",
    "        fake = fake+0.1\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.show_sentence(epoch)\n",
    "\n",
    "    def show_sentence(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_sentence = GAN2vec.generator.predict(noise)\n",
    "        test = np.squeeze(gen_sentence)\n",
    "        for i in test:\n",
    "            sentence = \"\"\n",
    "            for j in i:\n",
    "                temp = model.wv.similar_by_vector(j)\n",
    "                sentence=sentence+temp[0][0]+\" \"\n",
    "            print(sentence)\n",
    "            \n",
    "    def predict(self):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_sentence = GAN2vec.generator.predict(noise)\n",
    "        test = np.squeeze(gen_sentence)\n",
    "        sentence_list = []\n",
    "        for i in test:\n",
    "            sentence = \"\"\n",
    "            for j in i:\n",
    "                temp = model.wv.similar_by_vector(j)\n",
    "                sentence=sentence+temp[0][0]+\" \"\n",
    "            sentence_list.append(sentence)\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 5, 1, 256)         49408     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 5, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 1, 1, 128)         163968    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,505\n",
      "Trainable params: 213,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 3, 16, 256)        6291712   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 3, 16, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 7, 64, 1)          26113     \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 7, 64, 1)          0         \n",
      "=================================================================\n",
      "Total params: 6,371,585\n",
      "Trainable params: 6,370,561\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__index__ returned non-int (type NoneType)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-d380d7bb01ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mGAN2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGAN2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mGAN2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrain_D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mGAN2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-f2ab091002b4>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Trains the generator to fool the discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgumbel_softmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\notebook2\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                     output_loss = weighted_loss(y_true, y_pred,\n\u001b[1;32m--> 342\u001b[1;33m                                                 sample_weight, mask)\n\u001b[0m\u001b[0;32m    343\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\notebook2\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mweighted\u001b[1;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[0;32m    402\u001b[0m         \"\"\"\n\u001b[0;32m    403\u001b[0m         \u001b[1;31m# score_array has ndim >= 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m         \u001b[0mscore_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[1;31m# Cast the mask to floatX to avoid float64 upcasting in Theano\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-05313e3c948d>\u001b[0m in \u001b[0;36mgumbel_softmax\u001b[1;34m(logits, tau)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgumbel_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample_gumbel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __index__ returned non-int (type NoneType)"
     ]
    }
   ],
   "source": [
    "GAN2vec = GAN2vec()\n",
    "GAN2vec.pretrain_D(epochs = 100)\n",
    "GAN2vec.train(epochs=4000, batch_size=32, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend dance tanaka \n",
      "<s> i 'm glad lend someone tanaka \n",
      "<s> i 'm glad lend dance tanaka \n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    t = GAN2vec.predict()\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
